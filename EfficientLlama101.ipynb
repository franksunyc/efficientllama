{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Efficient Lamma Training 101 (Part 3): A llama that obeys your instruction.</h2>\n",
    "\n",
    "---\n",
    "<div align=\"center\">\n",
    "\n",
    "![](https://img.shields.io/badge/build-passing-green.svg)\n",
    "![](https://img.shields.io/badge/transformers-4.28.0-green.svg)\n",
    "![](https://img.shields.io/badge/version-1.1-blue.svg)\n",
    "![](https://img.shields.io/badge/python-%203.8%20|%203.9-blue.svg)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates Efficient Llama tuning using Lora and Meta-learning techniques.\n",
    "\n",
    "#### Limitations:\n",
    "1. The model was not trained on math calculations or summarization tasks, resulting in suboptimal performance in these areas. Instead, we incorporated title generation tasks during meta-training.\n",
    "2. If performance is unsatisfactory, consider prompt engineering, such as utilizing ChatGPT to refine your prompts for better results.\n",
    "3. This model is designed for zero-shot classification of marketing-related data; retraining may be necessary for optimal performance in your specific domain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up Environment\n",
    "#### 1.1 Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open this only if you need to install these packages\n",
    "\n",
    "# !pip install bitsandbytes datasets loralib sentencepiece tqdm\n",
    "\n",
    "# need the latest transformer to make Llama work (4.28.0 dev)\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# If this not working for you, try the original contributer's repo (https://github.com/huggingface/transformers/pull/21955)\n",
    "\n",
    "# for load efficient fine-tunning param\n",
    "# !pip install git+https://github.com/huggingface/peft.git\n",
    "\n",
    "# for pytorch, choose with caution\n",
    "# gpu version, use this if you have gpu and cuda ready on your computer\n",
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# cpu version\n",
    "# !pip install torch==1.13.1+cpu torchvision==0.14.1+cpu torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-4d644400-d6fc-49f7-b753-b420890e6176.sock\",\"/tmp/.X11-unix/X1\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
    "from peft import PeftModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# setup device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# setup tqdm        \n",
    "# tqdm.pandas()\n",
    "\n",
    "# if sentencepiece raise error, try to run it on cpu or a linux machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to base model, leave as blank if you want to download from HF\n",
    "llama_model_path = \"\"\n",
    "if not llama_model_path:\n",
    "    llama_model_path = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "# the path to the auxiliary model\n",
    "efficient_llama_model_path = \"\"\n",
    "\n",
    "if not efficient_llama_model_path:\n",
    "    raise Exception(\"Please input your auxiliary model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model and Setup template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# load llama model\n",
    "llama_tokenizer = LLaMATokenizer.from_pretrained(llama_model_path)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    # load the weights into gpu\n",
    "    # load base quantized model\n",
    "    llama_model = LLaMAForCausalLM.from_pretrained(llama_model_path, load_in_8bit=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    # load fine-tuned weights\n",
    "    llama_model = PeftModel.from_pretrained(llama_model, efficient_llama_model_path, torch_dtype=torch.float16)\n",
    "else:\n",
    "    # set the weights into cpu\n",
    "    device_map = {\"\": device}\n",
    "    # load base model\n",
    "    # if working on cpu then we want to shrink the memory usage\n",
    "    llama_model = LLaMAForCausalLM.from_pretrained(llama_model_path, device_map=device_map, low_cpu_mem_usage=True)\n",
    "    # load fine-tuned weights\n",
    "    llama_model = PeftModel.from_pretrained(llama_model, efficient_llama_model_path, device_map=device_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Human-instruction / Self-instruction template\n",
    "\n",
    "In this section, I will demonstrate how instructions are defined, which is crucial for composing your own instructions.\n",
    "\n",
    "- General question instructions:\n",
    "    Pose questions directly without any additional input.\n",
    "    ```yaml\n",
    "    Example without input:\n",
    "        instruction: What is the capital of France?\n",
    "        input: \n",
    "        output: The capital of France is Paris.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_only_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{_instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task-specific instruction and input:\n",
    "    For task-oriented questions, specify the task in the instruction and provide a sample in the input.\n",
    "    ```yaml\n",
    "    Sample with input:\n",
    "        instruction: Classify the following into animals, plants, and minerals\n",
    "        input: Oak tree, copper ore, elephant\n",
    "        output: Oak tree: Plant\\n Copper ore: Mineral\\n Elephant: Animal\\n\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_and_input_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{_instruction}\n",
    "\n",
    "### Input:\n",
    "{_input}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input(_instruction:str, _input:str) -> str:\n",
    "    \"\"\"This function gernerates a template to feed into LLM\n",
    "    input:\n",
    "        _instruction: string\n",
    "        _input: string\n",
    "    \n",
    "    return: \n",
    "        string\n",
    "    \"\"\"\n",
    "    if _input:\n",
    "        return instruction_and_input_template.format(_instruction=_instruction, _input=_input)\n",
    "    else:\n",
    "        return instruction_only_template.format(_instruction=_instruction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Model Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(_instruction:str, \n",
    "        _input:str=None, \n",
    "        temperature:float=0.1, \n",
    "        top_p:float=0.75, \n",
    "        num_beams:int=4,\n",
    "        max_len:int=256\n",
    ") -> str:\n",
    "    \"\"\"This function runs the model and return the output\n",
    "    input:\n",
    "        _instruction: string\n",
    "        _input: string\n",
    "\n",
    "        temperature: The value used to module the next token probabilities.\n",
    "        top_p: If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or higher are kept for generation.\n",
    "        num_beams: Number of beams for beam search. 1 means no beam search.\n",
    "        max_len: max generation length\n",
    "        \n",
    "    return: \n",
    "        string\n",
    "    \"\"\"\n",
    "    # \n",
    "    model_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams\n",
    "    )\n",
    "    \n",
    "    # get the instruction input\n",
    "    model_input = get_model_input(_instruction, _input)\n",
    "    \n",
    "    # tokenized input\n",
    "    model_input = llama_tokenizer(model_input, return_tensors=\"pt\")\n",
    "    model_input_ids = model_input[\"input_ids\"].to(device)\n",
    "\n",
    "    # infer only, do not compute gradient\n",
    "    with torch.no_grad():\n",
    "        model_output = llama_model.generate(\n",
    "            input_ids=model_input_ids,\n",
    "            generation_config=model_config,\n",
    "            max_new_tokens=max_len,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    return llama_tokenizer.decode(model_output.sequences[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Model - Zero Shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruction:\n",
      "We car dealership. Generate a marketing email to our customer and give them a 20 off for new year. Also promot our BHPH loan plan.\n",
      "\n",
      "Input:\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Response:\n",
      "Dear Valued Customer,\n",
      "\n",
      "Happy New Year! We hope you had a wonderful holiday season and are looking forward to a prosperous 2021. \n",
      "\n",
      "To celebrate the new year, we're offering 20% off on all new car purchases. We're also promoting our Buy Here, Pay Here (BHPH) loan plan, which allows you to pay for your car in affordable monthly installments. \n",
      "\n",
      "If you're interested in learning more about our BHPH loan plan or taking advantage of our 20% off offer, please don't hesitate to contact us. We look forward to hearing from you soon.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# understand domain specific words.\n",
    "_instruction = \"We car dealership. Generate a marketing email to our customer and give them a 20 off for new year. Also promot our   an.\"\n",
    "_input = \"\"\n",
    "\n",
    "# disambiguous\n",
    "# _instruction = \"Explain how 'mac' is used differently in these sentences?\"\n",
    "# _input = \"['I love big mac.', 'My mac is broken']\"\n",
    "\n",
    "# Classification\n",
    "# _instruction = \"Categorize the given sentence into the following categories.: Finance, Romantic, Retail, Food, and None of the above. Assign multiple categories if needed.\"\n",
    "# _input = \"What if we go to Macy's and grab some lunch at Chick-fil-A?\"\n",
    "\n",
    "# Extract entities\n",
    "# _instruction = \"Should I target or consider the user who sent the following message as my audience for promoting our new laptop product? Explain why.\"\n",
    "# _input = \"I just got my salary. I'll just save it for future usage.\"\n",
    "# _input = 'My macbook has just broken.'\n",
    "\n",
    "# Extract entities\n",
    "# _instruction = \"What is the life stage of the user who sent the following messages? Explain why. Life Stages: in college, married, have a baby, new house.\"\n",
    "# _input = \"We need get the car seats for her.\"\n",
    "# _input = 'I am a little bit nevers about going to Umass this summar.'\n",
    "\n",
    "print(run(_instruction, _input))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
